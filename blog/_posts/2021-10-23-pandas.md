---
layout: post
title: "Pandas Guide"
category: Blog
comments: true
---

__[pandas](https://pandas.pydata.org/)__ is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,
built on top of the Python programming language. This post is a practical guide for using the Python data analysis library. For a more comprehensive understanding, check the official [pandas documentation](https://pandas.pydata.org/docs/index.html). For a handy quick reference, check the official [pandas cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf). There is also a [Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html) for advanced strategies with examples and links.

## 1. Reading data

The first thing to get started with pandas is how to get data into it.

### 1.1 Tabular data

Tabular data is data that is structured into rows, each of which contains information about some thing.

#### 1.1.1 Manage delimited files

```python
import pandas as pd

# basic case
df = pd.read_csv('path/to/file.csv')

# use `sep` parameter to specify what the delimiting character to use, e.g., ";"
df = pd.read_csv('path/to/file.csv', sep=';')

# specify headers and rows to skip
df = pd.read_csv('path/to/file.csv', hearder=3, skiprows=3)
```

For more parameters, check out the [`pandas.read_csv` official documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)

#### 1.1.2 Manage Excel files

To read Excel files in Python, use the [`pandas.read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) function, it has many of the same parameters as `pandas.read_csv`.

#### 1.1.3 Manage databases

Python can be used in databases applications, the first step is to create a connection, check this [tutorial](https://www.w3schools.com/python/python_mysql_getstarted.asp) for example.

```python
# extract data from a table
df = pd.read_sql_table(table_name='table_name', con='postgres:///db_name')

# run a query
query = """
SELECT col_1,
    col_2,
From  table_name
WHERE col_3 > 4
"""
df = pd.read_sql_query(query, 'postgres:///db_name')
```

### 1.2 JSON data

[JSON](https://en.wikipedia.org/wiki/JSON) (JavaScript Object Notation) is plain text with the format of an object, it is oftern used to exchange data on the web.

```python
import pandas as pd

df = pd.read_json('data.json')

print(df.to_string()) # print the entire DataFrame
```

There are two more functions for reading JSON data:

- [`pandas.DataFrame.from_dict`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.from_dict.html) for dictionary by columns or by index allowing dtype specification

```python
>>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}
>>> pd.DataFrame.from_dict(data)
   col_1 col_2
0      3     a
1      2     b
2      1     c
3      0     d
```

- [`pandas.DataFrame.from_records`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.from_records.html) for a structured ndarray, sequence of tuples or dicts, or DataFrame

```python
>>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],
...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])
>>> pd.DataFrame.from_records(data)
   col_1 col_2
0      3     a
1      2     b
2      1     c
3      0     d
```

## 2. Basic data interrogation

After reading data in pandas DataFrame, the next step is to assess our data.

### 2.1 Dimensions

```python
>>> import pandas as pd
>>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],
...                    'col3': [5, 6]})
>>> df.shape # return a tuple contains the number of rows and columns
(2, 3)
>>> len(df) # return the number of rows
2
>>> df.dtypes # return the data type of each column
```

### 2.2 Samples

```python
df.head(n) # return the first n rows, 5 by default
df.tail(n) # return the last  n rows, 5 by default

# find the top n rows
df.sort_values('list_sort_by', ascending=False).head(n)
# find the top n columns
df.head().transpose()
```

### 2.3 Statistics

[How to calculate summary statistics?](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html)

The most basic way to summarize the data is the `describe()` function.

## 3. Filter data

Filtering data is one of the most common ways to interact with pandas DataFrame.

### 3.1 Columns

There are two ways to select columns in pandas: `df['column_name']` and `df.column_name`. For some reasons, the former is better.

To extract multiple columns, a list of column names is needed.

```python
df[['col_1', 'col_2', 'col_3']]
# doubele square brackets is needed

# to make it clearer
columns_to_extract = ['col_1', 'col_2', 'col_3']
df[columns_to_extract]
```

To select columns based on their position without knowing their name, we can use the [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) function.

```python
df.iloc[<row numbers>, <column numbers>]
```

### 3.2 Rows

#### 3.2.1 Select based on index

Every DataFrame has an index for rows by default, which works like names for columns. Use `df.index` to look at the index for the DataFrame `df`. The [`pandas.DataFrame.set_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html) function is used to set the DataFrame index using existing columns or arrays (of the correct length).

Equivalent to `iloc`, [`pandas.DataFrame.loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) access a group of rows and columns by label(s) or a boolean array.

```python
df.loc[<row indices>, <column names>]
```

Sometimes we just want to select some rows for all columns, the `<column names>` can be omitted, i.e., `df.loc[[1, 2, 3]]` is the same as `df.loc[[1, 2, 3], :]`.

#### 3.2.2 Select based on value

```python
# return all rows where the value in the "filter_col" column is greater than n
df[df['filter_col'] > n]
# repeat df is needed

# to make it clearer
filter = df['filter_col'] > n
# return a boolean series with the same length as the DataFrame
df[filter] # or df.loc[filter], return the rows with a True value

# select every 3rd rows
every_3rd_row = [i % 3 == 0 for i in range(len(df))]
df[every_3rd_row]
```

#### 3.2.3 Select based on multiple conditions

The following characters will be helpful.

| Character | Meaning       |
| --------- | ------------- |
| &         | AND condition |
| \|        | OR condition  |
| ~         | negation      |

```python
filter = ((df['col_1'] > a) & (df['col_2'] < b)) | (df['col_3'] == c)
df.loc[filter, ['col_4', 'col_5']]
```

#### 3.2.4 Select based on advanced booleans

- `df.isin()` returns whether each element in the DataFrame is contained in values.
- `df.isna()` mask of bool values for each element in DataFrame that indicates whether an element is an NA value.
- [`pandas.Series.between`](https://pandas.pydata.org/docs/reference/api/pandas.Series.between.html) return boolean Series equivalent to left <= series <= right.
- [Working with text data](https://pandas.pydata.org/docs/user_guide/text.html)
- [Time series / date functionality](https://pandas.pydata.org/docs/user_guide/timeseries.html)

## 4. Aggregation

Aggregation for efficient summarization is an essential piece of analysis of large data.

### 4.1 Basics

#### 4.1.1 Most common methods

Most used for numeric data.

- `count()` — count non-NA cells for each column(default) or row (set axis=1)
- `nunique()` — count number of distinct elements
- `sum()`/`mean()` — return the sum/mean of the values
- `min()`/`max()`/`median()` — return the minimum/maximum/median of the values
- `quantile()` — return values at the given quantile, defaults to the 50th quantile (q=0.5, the median)
- `var()`/`std()` — return (unbaised variance)/(sample standard deviation) of the values. Normalized by N-1(the sample formulation) by default, set `ddof=0`(N-ddof) for population formulation.

#### 4.1.2 Using `groupby()`

When doing aggregation, more oftern than not, we would like to analyze data by some categories. `groupby()` allows us to specify a column (or multiple columns) to aggregate the values for better analysis.

The name GroupBy should be quite familiar to those who have used a SQL-based tool, in which you can write code like:

```sql
SELECT Column1, Column2, mean(Column3), sum(Column4)
FROM SomeTable
GROUP BY Column1, Column2
```

Let's take a simple example.

```python
>>>df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',
...                               'Parrot', 'Parrot'],
...                    'Max Speed': [380., 370., 24., 26.]})
>>> df
   Animal  Max Speed
0  Falcon      380.0
1  Falcon      370.0
2  Parrot       24.0
3  Parrot       26.0
>>> df.groupby(['Animal']).mean()
        Max Speed
Animal
Falcon      375.0
Parrot       25.0
```

- `df.groupby('A')` is just syntactic sugar for `df.groupby(df['A'])`.
- We can groupby different levels of a hierarchical index using the `level` parameter.
- We can also choose to include NA in group keys or not by setting `dropna`(default is *True*) parameter.

#### 4.1.3 Using `agg()`

If we want to aggregate using one or more operations and have one or more columns to aggragate, then we can use the `agg()` (*agg* is an alias for *aggragate*) method.

```python
>>>df = pd.DataFrame([[1, 2, 3],
...                    [4, 5, 6],
...                    [7, 8, 9],
...                    [np.nan, np.nan, np.nan]],
...                   columns=['A', 'B', 'C'])

# Aggregate these functions over the rows.
>>>df.agg(['sum', 'min'])
        A     B     C
sum  12.0  15.0  18.0
min   1.0   2.0   3.0

# Different aggregations per column.
>>>df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})
        A    B
sum  12.0  NaN
min   1.0  2.0
max   NaN  8.0

# Aggregate different functions over the columns and rename the index of the resulting DataFrame.
>>>df.agg(x=('A', 'max'), y=('B', 'min'), z=('C', np.mean))
     A    B    C
x  7.0  NaN  NaN
y  NaN  2.0  NaN
z  NaN  NaN  6.0
```

#### 4.1.4 Transform

To be added.
